@article{Atkinson2008,
author = {Atkinson, Elizabeth J. and Crowson, Cythia S. and Pedersen, Rachel A. and Therneau, Terry M.},
journal = {Technical report series No. 81, Department of Health Sciences Research, Rochester, MN, USA: Mayo Clinic},
title = {{Poisson models for person-years and expected rates}},
year = {2008}
}
@article{Austin2020,
abstract = {In the context of survival analysis, calibration refers to the agreement between predicted probabilities and observed event rates or frequencies of the outcome within a given duration of time. We aimed to describe and evaluate methods for graphically assessing the calibration of survival models. We focus on hazard regression models and restricted cubic splines in conjunction with a Cox proportional hazards model. We also describe modifications of the Integrated Calibration Index, of E50 and of E90. In this context, this is the average (respectively, median or 90th percentile) absolute difference between predicted survival probabilities and smoothed survival frequencies. We conducted a series of Monte Carlo simulations to evaluate the performance of these calibration measures when the underlying model has been correctly specified and under different types of model mis-specification. We illustrate the utility of calibration curves and the three calibration metrics by using them to compare the calibration of a Cox proportional hazards regression model with that of a random survival forest for predicting mortality in patients hospitalized with heart failure. Under a correctly specified regression model, differences between the two methods for constructing calibration curves were minimal, although the performance of the method based on restricted cubic splines tended to be slightly better. In contrast, under a mis-specified model, the smoothed calibration curved constructed using hazard regression tended to be closer to the true calibration curve. The use of calibration curves and of these numeric calibration metrics permits for a comprehensive comparison of the calibration of competing survival models.},
author = {Austin, Peter C. and Harrell, Frank E. and van Klaveren, David},
doi = {10.1002/sim.8570},
file = {:P\:/Documents/JournalsandPapers/Austin2019.pdf:pdf},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {calibration,model validation,random forests,survival analysis,time-to-event model},
number = {21},
pages = {2714--2742},
pmid = {32548928},
title = {{Graphical calibration curves and the integrated calibration index (ICI) for survival models}},
volume = {39},
year = {2020}
}
@article{Austin2022,
abstract = {Assessing calibration—the agreement between estimated risk and observed proportions—is an important component of deriving and validating clinical prediction models. Methods for assessing the calibration of prognostic models for use with competing risk data have received little attention.},
author = {Austin, Peter C. and Putter, Hein and Giardiello, Daniele and van Klaveren, David},
doi = {10.1186/s41512-021-00114-6},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Austin2022.pdf:pdf},
isbn = {4151202100114},
journal = {Diagnostic and Prognostic Research},
keywords = {Calibration,Competing risks,Survival analysis,Time,calibration,competing risks,model validation,random forests,survival analysis,time-to-event model},
number = {1},
publisher = {Diagnostic and Prognostic Research},
title = {{Graphical calibration curves and the integrated calibration index (ICI) for competing risk models}},
volume = {6},
year = {2022}
}
@article{Berry1983,
author = {Berry, G},
journal = {Biometrics},
number = {173-184},
title = {{The analysis of mortality by the subject-years method}},
volume = {39},
year = {1983}
}
@book{Therneau2000,
author = {Therneau, Terry M and Grambsch, Patricia M},
doi = {10.1007/978-1-4757-3294-8},
publisher = {Springer New York, NY},
title = {{Modeling Survival Data: Extending the Cox Model}},
year = {2000}
}
@article{Balan2019,
abstract = {Multivariate survival data are frequently encountered in biomedical applications in the form of clustered failures (or recurrent events data). A popular way of analyzing such data is by using shared frailty models, which assume that the proportional hazards assumption holds conditional on an unobserved cluster-specific random effect. Such models are often incorporated in more complicated joint models in survival analysis. If the random effect distribution has finite expectation, then the conditional proportional hazards assumption does not carry over to the marginal models. It has been shown that, for univariate data, this makes it impossible to distinguish between the presence of unobserved heterogeneity (eg, due to missing covariates) and marginal nonproportional hazards. We show that time-dependent covariate effects may falsely appear as evidence in favor of a frailty model also in the case of clustered failures or recurrent events data, when the cluster size or number of recurrent events is small. When true unobserved heterogeneity is present, the presence of nonproportional hazards leads to overestimating the frailty effect. We show that this phenomenon is somewhat mitigated as the cluster size grows. We carry out a simulation study to assess the behavior of test statistics and estimators for frailty models in such contexts. The gamma, inverse Gaussian, and positive stable shared frailty models are contrasted using a novel software implementation for estimating semiparametric shared frailty models. Two main questions are addressed in the contexts of clustered failures and recurrent events: whether covariates with a time-dependent effect may appear as indication of unobserved heterogeneity and whether the additional presence of unobserved heterogeneity can be detected in this case. Finally, the practical implications are illustrated in a real-world data analysis example.},
author = {Balan, Theodor Adrian and Putter, Hein},
doi = {10.1002/sim.8171},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Balan2019v2.pdf:pdf},
issn = {10970258},
journal = {Statistics in Medicine},
keywords = {frailty,proportional hazards,unobserved heterogeneity},
number = {18},
pages = {3405--3420},
pmid = {31050028},
title = {{Nonproportional hazards and unobserved heterogeneity in clustered survival data: When can we tell the difference?}},
volume = {38},
year = {2019}
}
@article{Vaupel2024,
author = {Vaupel, James W and Manton, Kenneth G and Stallard, Eric},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Vaupel1979.pdf:pdf},
journal = {Demography},
number = {3},
pages = {439--454},
title = {{The Impact of Heterogeneity in Individual Frailty on the Dynamics of Mortality}},
url = {https://www.jstor.org/stable/2061224},
volume = {16},
year = {1979}
}
@article{Lin2013,
abstract = {Summary: Omission of relevant covariates can lead to bias when estimating treatment or exposure effects from survival data in both randomized controlled trials and observational studies. This paper presents a general approach to assessing bias when covariates are omitted from the Cox model. The proposed method is applicable to both randomized and non-randomized studies. We distinguish between the effects of three possible sources of bias: omission of a balanced covariate, data censoring and unmeasured confounding. Asymptotic formulae for determining the bias are derived from the large sample properties of the maximum likelihood estimator. A simulation study is used to demonstrate the validity of the bias formulae and to characterize the influence of the different sources of bias. It is shown that the bias converges to fixed limits as the effect of the omitted covariate increases, irrespective of the degree of confounding. The bias formulae are used as the basis for developing a new method of sensitivity analysis to assess the impact of omitted covariates on estimates of treatment or exposure effects. In simulation studies, the proposed method gave unbiased treatment estimates and confidence intervals with good coverage when the true sensitivity parameters were known. We describe application of the method to a randomized controlled trial and a non-randomized study. {\textcopyright} 2013 The Authors. Biometrics published by The International Biometric Society.},
author = {Lin, Nan Xuan and Logan, Stuart and Henley, William Edward},
doi = {10.1111/biom.12096},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Lin2013.pdf:pdf},
issn = {15410420},
journal = {Biometrics},
keywords = {Bias analysis,Cox model,Omitted covariates,Sensitivity analysis,Survival analysis,Unmeasured confounding},
number = {4},
pages = {850--860},
pmid = {24224574},
title = {{Bias and sensitivity analysis when estimating treatment effects from the cox model with omitted covariates}},
volume = {69},
year = {2013}
}
@article{Crowson2016,
abstract = {Current methods used to assess calibration are limited, particularly in the assessment of prognostic models. Methods for testing and visualizing calibration (e.g. the Hosmer-Lemeshow test and calibration slope) have been well thought out in the binary regression setting. However, extension of these methods to Cox models is less well known and could be improved. We describe a model-based framework for the assessment of calibration in the binary setting that provides natural extensions to the survival data setting. We show that Poisson regression models can be used to easily assess calibration in prognostic models. In addition, we show that a calibration test suggested for use in survival data has poor performance. Finally, we apply these methods to the problem of external validation of a risk score developed for the general population when assessed in a special patient population (i.e. patients with particular comorbidities, such as rheumatoid arthritis).},
author = {Crowson, Cynthia S. and Atkinson, Elizabeth J. and Therneau, Terry M. and Lawson, Andrew B. and Lee, Duncan and MacNab, Ying},
doi = {10.1177/0962280213497434},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Crowson2013.pdf:pdf},
issn = {14770334},
journal = {Statistical Methods in Medical Research},
keywords = {Cox model,Poisson,calibration,prognostic risk scores,standardized incidence ratio,survival},
number = {4},
pages = {1692--1706},
pmid = {23907781},
title = {{Assessing calibration of prognostic risk scores}},
volume = {25},
year = {2016}
}
@article{Martinussen2013,
abstract = {We study the situation where it is of interest to estimate the effect of an exposure variable X on a survival time response T in the presence of confounding by measured variables Z. Quantifying the amount of confounding is complicated by the non-collapsibility or non-linearity of typical effect measures in survival analysis: survival analyses with or without adjustment for Z typically infer different effect estimands of a different magnitude, even when Z is not associated with the exposure, and henceforth not a confounder of the association between exposure and survival time. We show that, interestingly, the exposure coefficient indexing the Aalen additive hazards model is not subject to such non-collapsibility, unlike the corresponding coefficient indexing the Cox model, so that simple measures of the amount of confounding bias are obtainable for the Aalen hazards model, but not for the Cox model. We argue that various other desirable properties can be ascribed to the Aalen model as a result of this collapsibility. This work generalizes recent work by Janes et al. (Biostatistics 11:572-582, 2010). {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Martinussen, Torben and Vansteelandt, Stijn},
doi = {10.1007/s10985-013-9242-z},
file = {:/nask.man.ac.uk/home$/Documents/JournalsandPapers/Martinussen2013.pdf:pdf},
issn = {13807870},
journal = {Lifetime Data Analysis},
keywords = {Aalen's additive model,Causal effect,Collapsibility,Confounding,Cox model},
number = {3},
pages = {279--296},
pmid = {23329123},
title = {{On collapsibility and confounding bias in Cox and Aalen regression models}},
volume = {19},
year = {2013}
}
